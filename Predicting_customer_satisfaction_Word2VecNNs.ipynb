{"cells":[{"cell_type":"markdown","metadata":{"id":"FFVtwBwG7W3b"},"source":["# Customer Satisfaction Prediction - Brazillian e-Commerce Public Dataset"]},{"cell_type":"markdown","metadata":{"id":"XC_thK_Z7W3d"},"source":["## 1. Business Problem:-"]},{"cell_type":"markdown","metadata":{"id":"FN6tJc-Q7W3f"},"source":["### 1.1 Description\n","\n","This is a Brazilian ecommerce public dataset of orders made at Olist Store. The dataset has information of 100k orders from 2016 to 2018 made at multiple marketplaces in Brazil. Its features allows viewing an order from multiple dimensions: from order status, price, payment and freight performance to customer location, product attributes and finally reviews written by customers. A geolocation dataset that relates Brazilian zip codes to lat/lng coordinates has also been released.\n","\n","This dataset was generously provided by Olist, the largest department store in Brazilian marketplaces. Olist connects small businesses from all over Brazil to channels without hassle and with a single contract. Those merchants are able to sell their products through the Olist Store and ship them directly to the customers using Olist logistics partners. See more on the website: www.olist.com\n","\n","After a customer purchases the product from Olist Store a seller gets notified to fulfill that order. Once the customer receives the product, or the estimated delivery date is due, the customer gets a satisfaction survey by email where he can give a note for the purchase experience and write down some comments.\n","\n","CREDITS:- Kaggle"]},{"cell_type":"markdown","metadata":{"id":"QPi_gAqc7W3g"},"source":["### 1.2 Problem Statement\n","Predict Customer satisfaction of the purhase from the olist e-commerce site."]},{"cell_type":"markdown","metadata":{"id":"SbZk8q_J7W3i"},"source":["## 2. Machine Learning Probelm\n","### 2.1 Data\n","#### 2.1.1 Data Overview"]},{"cell_type":"markdown","metadata":{"id":"mOns1YUc7W3i"},"source":["Source:- https://www.kaggle.com/olistbr/brazilian-ecommerce\n","\n","The data is divided in multiple datasets for better understanding and organization. Please refer to the following data schema when working with it:\n","<img src=\"https://i.imgur.com/HRhd2Y0.png\" />\n"]},{"cell_type":"markdown","metadata":{"id":"3MF-dacV7W3j"},"source":["#### 2.1.2 Data Description\n","The **olist_orders_dataset** have the order data for each purchase connected with other data using order_id and customer_id.\n","The **olist_order_reviews_dataset** have the labeled review data for each order in the order data table labelled as [1,2,3,4,5] where 5 being the highest and 1 being the lowest.\n","We will use reviews greater than 3 as positive and less than equal to 3 as negative review.\n","The table will be joined accordingly to get the data needed for the analysis, feature selection and model training."]},{"cell_type":"markdown","metadata":{"id":"LX3pADHM7W3j"},"source":["### 2.2 Mapping the real world problem to an ML problem\n","#### 2.2.1 Type of Machine Leaning Problem\n","It is a binary classification problem, for a given purchase order we need to predict if it will get a positive or negative review from the customer."]},{"cell_type":"markdown","metadata":{"id":"ol6jwiuO7W3k"},"source":["#### 2.2.2 Performance Metric"]},{"cell_type":"markdown","metadata":{"id":"PKAPJc7P7W3k"},"source":["Metric(s):\n","* f1-score : https://www.kaggle.com/wiki/LogarithmicLoss\n","* Binary Confusion Matrix"]},{"cell_type":"markdown","metadata":{"id":"jJepcmoV7W3l"},"source":["### 2.3 Train and Test Construction\n","\n","We build train and test by stratified random split of the data in the ratio of 70:30 or 80:20 whatever we choose as we have sufficient points to work with."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcX5zu0_7iMz","executionInfo":{"status":"ok","timestamp":1714402029770,"user_tz":-420,"elapsed":25394,"user":{"displayName":"Pham Xuan Loc","userId":"16579008098910718427"}},"outputId":"7c477701-307f-49f8-f2cb-8c10e76446c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"-iWRbTPt7W3l"},"source":["## 3. Exploratory Data Analysis"]},{"cell_type":"markdown","metadata":{"id":"1PQiQMoj7W3m"},"source":["### 3.1 Importing libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"giu709Kp7W3m","executionInfo":{"status":"ok","timestamp":1714402038253,"user_tz":-420,"elapsed":8489,"user":{"displayName":"Pham Xuan Loc","userId":"16579008098910718427"}},"outputId":"4a8bbb13-6ee9-451a-92cb-038dea4fcd48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}],"source":["!pip install --upgrade gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quAPdHC07W3n","executionInfo":{"status":"ok","timestamp":1714402042825,"user_tz":-420,"elapsed":4578,"user":{"displayName":"Pham Xuan Loc","userId":"16579008098910718427"}},"outputId":"46c4b9fb-f499-449c-d5e8-754bedbcce71"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package rslp to /root/nltk_data...\n","[nltk_data]   Unzipping stemmers/rslp.zip.\n"]}],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","import re\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('rslp')\n","from nltk.corpus import stopwords\n","from nltk.stem import RSLPStemmer\n","from tqdm import tqdm\n","import shutil\n","import os\n","import numpy as np\n","import pandas as pd\n","import datetime as dt\n","import matplotlib\n","matplotlib.use(u'nbAgg')\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","import pickle\n","import random\n","from scipy.stats import randint as sp_randint\n","from scipy.stats import uniform\n","from sklearn.preprocessing import Normalizer\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from scipy.sparse import hstack\n","from xgboost import XGBClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import SGDClassifier,LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier, StackingClassifier, VotingClassifier, AdaBoostClassifier\n","from sklearn.metrics import log_loss,accuracy_score, confusion_matrix, f1_score"]},{"cell_type":"markdown","metadata":{"id":"Mf3g-km67W3o"},"source":["### 3.2 Loading data and preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lvENiwE7W3p","executionInfo":{"status":"error","timestamp":1714402042825,"user_tz":-420,"elapsed":33,"user":{"displayName":"Pham Xuan Loc","userId":"16579008098910718427"}},"colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"3e3a43c6-1089-4b6c-9dd4-8465a0f47922"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_customers_dataset.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-001c65959736>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading the data tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcustomer_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_customers_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgeolocation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_geolocation_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0morder_items_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_order_items_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0morder_payments_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_order_payments_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_customers_dataset.csv'"]}],"source":["# loading the data tables\n","customer_data = pd.read_csv('/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_customers_dataset.csv')\n","geolocation_data = pd.read_csv('/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_geolocation_dataset.csv')\n","order_items_dataset = pd.read_csv('/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_order_items_dataset.csv')\n","order_payments_dataset = pd.read_csv('/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_order_payments_dataset.csv')\n","order_reviews_dataset = pd.read_csv('/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_order_reviews_dataset.csv')\n","order_dataset = pd.read_csv('/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_orders_dataset.csv')\n","order_products_dataset = pd.read_csv('/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_products_dataset.csv')\n","order_sellers_dataset = pd.read_csv('/content/drive/MyDrive/Predict Customer Satisfaction /Data/olist_sellers_dataset.csv')\n","product_translation_dataset = pd.read_csv('/content/drive/MyDrive/Predict Customer Satisfaction /Data/product_category_name_translation.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ky_uzvL27W3p"},"outputs":[],"source":["# checking customer data\n","customer_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c56HrQ2-7W3q"},"outputs":[],"source":["# checking geo-location data\n","geolocation_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOtYF8Z37W3q"},"outputs":[],"source":["# checking ordered items data\n","order_items_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-4mtIL_h7W3q"},"outputs":[],"source":["# checking payments data\n","order_payments_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHAzqJut7W3r"},"outputs":[],"source":["# checking order reviews data\n","order_reviews_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUGbpdk77W3r"},"outputs":[],"source":["# checking the order data\n","order_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"33zxCEuF7W3s"},"outputs":[],"source":["# checking sellers data\n","order_sellers_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RykBRiQv7W3s"},"outputs":[],"source":["# checking products data\n","order_products_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIlBLHaX7W3t"},"outputs":[],"source":["# prdouct name translation data from Portugese to English\n","product_translation_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YOqMw5Xj7W3u"},"outputs":[],"source":["# checking info of reviews data\n","print(order_reviews_dataset.info())"]},{"cell_type":"markdown","metadata":{"id":"8kFjTDFa7W3u"},"source":["Here, we can see that the review data have review score for each 100k data points but less than 50% of the orders have review comments for them. Also, we want to predict customer review based on the order fullfillment rather than classifying their reviews as positive or negative based on the review comments posted by them.\n","\n","According to our objective here i.e to predict the customer satisfaction based on the order fullfillment rather than classifying their reviews as negative or positive, the review comments given by the customer should be removed from the data to avoid bias in the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ol_HejQ37W3u"},"outputs":[],"source":["# removing unuseful data from review data set\n","order_reviews_dataset = order_reviews_dataset[['order_id','review_score', 'review_comment_message']]\n","order_reviews_dataset.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwkLcTih7W3v"},"outputs":[],"source":["# Merging order data with review data to get a review score on each order\n","order_review_data = order_reviews_dataset.merge(order_dataset,on='order_id')\n","order_review_data.head()"]},{"cell_type":"markdown","metadata":{"id":"YksYBxu97W3v"},"source":["As seen above the product dataset containd the product categories in portugese language. so, let's translate the product categories to english for better understanding."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVqmdtlt7W3w"},"outputs":[],"source":["# changing product name to english in the ordered product dataset\n","order_products_dataset_english = pd.merge(order_products_dataset,product_translation_dataset,on='product_category_name'\n","                                          ,how='left')\n","order_products_dataset_english = order_products_dataset_english.drop(labels='product_category_name',axis=1)\n","order_products_dataset_english.head()"]},{"cell_type":"markdown","metadata":{"id":"TIfjkZ4D7W3x"},"source":["The above data set contains detailed description of each product item available for sale on website. So, let's merge this data with **order_items_dataset** which contains order details of each product item sold to get the product description of each item sold in the same data.\n","\n","The dataset now contains the detailed description of each product ordered online such as price, dimensions, seller, number of photos available to customer, and product weight etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXTogNse7W3x"},"outputs":[],"source":["# merging item description to the products ordered data using product_id\n","order_product_item_dataset = pd.merge(order_items_dataset,order_products_dataset_english,on='product_id')\n","order_product_item_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsU7c7PP7W3y"},"outputs":[],"source":["# merging detailed product data with the order review data\n","ordered_product_reviews = pd.merge(order_product_item_dataset,order_review_data,on='order_id')\n","ordered_product_reviews_payments = pd.merge(ordered_product_reviews,order_payments_dataset,on='order_id')\n","ordered_product_reviews_payments.head()"]},{"cell_type":"markdown","metadata":{"id":"V28FrvE27W3z"},"source":["#### 3.2.1 Final data\n","\n","Now, we have our final data set for each order_id for we have products info, sellers info, items info, customer info, payment info and review score given by the customer.\n","Let us dive deep into our data set and see what the data tells. let us start with simple statistics on the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qXCBIGN_7W30"},"outputs":[],"source":["# merging detailed product data with the order review data\n","df_final = pd.merge(ordered_product_reviews_payments,customer_data,on='customer_id')\n","# df_final.to_csv('olist_final.csv',index=False)\n","df_final.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYXIe4Jz7W30"},"outputs":[],"source":["#info on the data set\n","df_final.info()"]},{"cell_type":"markdown","metadata":{"id":"buWGaSIM7W31"},"source":["#### 3.2.2 Handling missing values\n","From above info table we can see that our data set have missing values for some of the features. let us see the statistics of missing values for each feature."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDxKcMze7W31"},"outputs":[],"source":["# checking the count of null values per column\n","df_final.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"qpe8NSNt7W32"},"source":["The maximum missing values is seen in the order delivery date feature of the data set with around 2% of the total data. For the numerical features with null values we will use median impute technique( to avoid outliers) to handle missing value of these columns. For the date column order delivery date and order approve date we will fill the missing value from the corresponfiing estimated delivery date column and order purchase time column. The customer generally does not pay attention to the order_delivered_carrier_date of their order. so, we will drop this column. Also, the categorical product category feature have null values less than 1% of total data so, we will drop those rows having null values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TwYrZUck7W32"},"outputs":[],"source":["# Handling missing values\n","df_final['product_name_lenght'].fillna(df_final['product_name_lenght'].median(),inplace=True)\n","df_final['product_description_lenght'].fillna(df_final['product_description_lenght'].median(),inplace=True)\n","df_final['product_photos_qty'].fillna(df_final['product_photos_qty'].median(),inplace=True)\n","df_final['product_weight_g'].fillna(df_final['product_weight_g'].median(),inplace=True)\n","df_final['product_length_cm'].fillna(df_final['product_length_cm'].median(),inplace=True)\n","df_final['product_height_cm'].fillna(df_final['product_height_cm'].median(),inplace=True)\n","df_final['product_width_cm'].fillna(df_final['product_width_cm'].median(),inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYN-GAPO7W32"},"outputs":[],"source":["#Handling missing values\n","ids = (df_final[df_final['order_delivered_customer_date'].isnull() == True].index.values)\n","vals = df_final.iloc[ids]['order_estimated_delivery_date'].values\n","df_final.loc[ids,'order_delivered_customer_date'] = vals\n","\n","ids = (df_final[df_final['order_approved_at'].isnull() == True].index.values)\n","df_final.loc[ids,'order_approved_at'] = df_final.iloc[ids]['order_purchase_timestamp'].values\n","\n","#dropping order delivery carrier date\n","df_final.drop(labels='order_delivered_carrier_date',axis=1,inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYnyy93B7W33"},"outputs":[],"source":["# filling nan value of review comments with no_review\n","df_final['review_comment_message'].fillna('no_review',inplace=True)\n","\n","# dropping rows with product category name as null\n","df_final = df_final.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYEx7OfU7W33"},"outputs":[],"source":["df_final.info()"]},{"cell_type":"markdown","metadata":{"id":"7BUYrN9B7W34"},"source":["##### Observation\n","We have observed different ways of handling missing features based on the features types and missing values. Missing value in the numerical features were handled using the imputation technique through median. While for the features with delivery dates we handled it differently. We used the data from other columns to fill missing values for these features like we assumed that any order with missing order customer delivery date should have been delivered by estimated delivery date and so we filled it in the same way. As for the features with less than 1% missing values we dropped the data points containing any null values.\n","Now, that we have handled all our missing values in the data and we can say that the preprocessing of the data is complete.so, let us go ahead and do some analysis on the data.\n","\n","### 3.3 Data Analysis\n","Since preprocessig of the data is done and now we have our final data set with us. Let us analyse our data and find meaningful insights from the data.\n","\n","Our objective here is to build a model which can predict the review score on the data or classify the data into 0 and 1 review score. So, let us try to find insights and analyze the data keeping our objective in mind."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cw1aTPgp7W34"},"outputs":[],"source":["# checking the review score\n","df_final.review_score.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"Le4cgPO-7W35"},"source":[">According to our objective, we are going to solve this problem using binary classification technique. so, let us convert the review score into 0 and 1 labels and view the distribution."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QaENfqIR7W35"},"outputs":[],"source":["# converting reviews into 0 and 1 to make it binary classification problem\n","df_final['review_score'] = df_final['review_score'].apply(lambda x:1 if x>3 else 0)\n","\n","#let us see the distribution now.\n","plt.figure(figsize=(10,5))\n","ax=sns.countplot(x=\"review_score\", data=df_final)\n","plt.title('Distribution of Review Score')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Yl2Jaspl7W36"},"source":["###### Observation\n","The above plot show the distribution of the class labels(review_score) in the data set. From the plot, we can see that more than 50% of the data points belong to the class label 1 i.e positive class and rest of them to the negative class suggesting that we have class imbalanced data set."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"IRyeGwT27W36"},"outputs":[],"source":["# statistics of numerical features in the data set\n","df_final.describe()"]},{"cell_type":"markdown","metadata":{"id":"pnA7X7147W37"},"source":["###### Observation\n","We observe from above table that we have 12 useful numerical features except zip code, our target variable review score and order_item_id. let us observe the statistics of the price and freight value of an order. The maximum price of an order is 6735 while max freight went to around 410 Brazilian real. The average price of an order is around 120 Brazilian real and frieght value is around 20 Brazilian real. The order with minimum price of 0.85 Brazilian real have been made. Let us look at distribution of these features and see how they help in classifying the class labels and find other insights."]},{"cell_type":"markdown","metadata":{"id":"fO-cRpCo7W37"},"source":["#### 3.3.1 Univariate Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmjwhkZL7W38"},"outputs":[],"source":["# https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\n","# plotting distributions of price per class\n","plt.figure()\n","sns.set_style(\"whitegrid\")\n","ax = sns.FacetGrid(df_final, hue=\"review_score\", height=5,aspect=2.0)\n","ax = ax.map(sns.distplot, \"price\").add_legend();\n","plt.title('Distribution of product price per class')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"67_ihANg7W38"},"source":["###### Observation\n","The distribution plot above shows the distribution of price for both the postive and negative classes. The overlap of both the distribution for positive and negative class suggests that it is not possible to classify them based only on price feature."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dWBgVI007W39"},"outputs":[],"source":["# plotting distributions of freight_value per class\n","plt.figure()\n","sns.set_style(\"whitegrid\")\n","ax = sns.FacetGrid(df_final, hue=\"review_score\", height=5,aspect=2.0)\n","ax = ax.map(sns.distplot, \"freight_value\").add_legend();\n","plt.title('Distribution of freight_value per class')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"VJD04Mgo7W39"},"source":["##### Observation\n","From the plot above titled `Distribution plot of freight value`, we observe that freight value is somewhat normally distributed but it too is overlapping for both the classes and hence provide much info in the classification.\n","Let us look at some more distributions and see if we get something important from any of them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cuUdtEt_7W39"},"outputs":[],"source":["# plotting distributions of product_height_cm per class\n","sns.set_style(\"whitegrid\")\n","ax = sns.FacetGrid(df_final, hue=\"review_score\", height=5,aspect=2.0)\n","ax = ax.map(sns.distplot, \"product_height_cm\").add_legend();\n","plt.title('Distribution of product_height_cm per class')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1YohbM_7W3-"},"outputs":[],"source":["# distriution plot of product_weight_g\n","plt.figure()\n","sns.set_style(\"whitegrid\")\n","ax = sns.FacetGrid(df_final, hue=\"review_score\", height=5,aspect=2.0)\n","ax = ax.map(sns.distplot, \"product_weight_g\").add_legend();\n","plt.title('Distribution of product_weight_g per class')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KjfeEELu7W3_"},"outputs":[],"source":["# distriution plot of payment_value\n","plt.figure()\n","sns.set_style(\"whitegrid\")\n","ax = sns.FacetGrid(df_final, hue=\"review_score\", height=5,aspect=2.0)\n","ax = ax.map(sns.distplot, \"payment_value\").add_legend();\n","plt.title('Distribution of payment_value per class')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pYpQL3Ca7W3_"},"source":["##### Observation\n","From all the above univariate plots, we observed that almost all of them have overlapping distributions for the class labels. We can infer from that values of features lying in any range of their distribution have almost equal chance of gettig a postive or negative review. So, if that is the case how can we train the model to classify the positive and negative points if we cannot properly differentiate based on the features value(distribution).\n","So, let us go and do some bivariate analysis and see if we use more than one feature at time, can we come with something to classify these features."]},{"cell_type":"markdown","metadata":{"id":"AvYai6Ax7W3_"},"source":["#### 3.3.2 Bivariate Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tni-vcBf7W3_"},"outputs":[],"source":["# Distribution of price vs freight_value per class\n","plt.figure(figsize=(8,5))\n","sns.set_style(\"whitegrid\")\n","ax = sns.scatterplot(x='price',y='freight_value', data = df_final, hue=\"review_score\")\n","plt.title('Distribution of price vs freight_value per class')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ezEgbz6T7W4A"},"outputs":[],"source":["# Distribution of price vs freight_value per class\n","plt.figure(figsize=(8,5))\n","sns.set_style(\"whitegrid\")\n","ax = sns.scatterplot(x='price',y='product_weight_g', data = df_final, hue=\"review_score\")\n","plt.title('Distribution of price vs freight_value per class')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"DFPLZTJ57W4A"},"source":["##### Observation\n","From the above two scatter plots titled `Distribution of price vs freight_value per class` and `Distribution of price vs freight_value per class` respectively, we tried to observe how are the features price and freight_value distributed for the class labels in the first plot. From the plots we observe that the points are mixed together for the both the classes suggesting us that algorithms like KNN might not be good in classifying these points. Therefore, we will observe the distribution of few more features with each other and see we find something more important."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQc5k-pc7W4B"},"outputs":[],"source":["# https://seaborn.pydata.org/generated/seaborn.pairplot.html\n","# pair plot\n","sns.set(style=\"ticks\", color_codes=True)\n","g = sns.pairplot(df_final[['price','freight_value','product_photos_qty','product_weight_g','product_length_cm',\n","                           'product_height_cm','product_width_cm', 'review_score']],hue='review_score')\n","# g.savefig(\"pairplot1.png\")"]},{"cell_type":"markdown","metadata":{"id":"uDgKlqam7W4B"},"source":["##### Observation\n","In the pair plot above we see distribution of one feature with rest of them for each class labels. In the pair plot above, we can see some of the blue points from the orange points while in the univariate analysis we observed overlapping for amost all the cases. The distribution of freight value with all the other features like price, product length etc. shows that we might be able to classify positive and negative class using some non-linear techniques where the univariate analysis showed that it is nearly impossible to classify them based on some straight forward conditions or linear way. The results in the pair plot are better than univariate analysis but it is not promising to come any sure shot conclusion so, in next step we can engineer some new features on the data and try to see if we can find any relations td differentiate between positive and negative class."]},{"cell_type":"markdown","metadata":{"id":"13xuEizB7W4C"},"source":["#### 3.3.3 Analysis of Categorical Variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21rYU6Yi7W4C"},"outputs":[],"source":["# count plot of payment type\n","# https://stackoverflow.com/questions/34615854/seaborn-countplot-with-normalized-y-axis-per-group\n","plt.figure(figsize=(8,5))\n","sns.set_style(\"whitegrid\")\n","ax = sns.countplot(x=\"review_score\", hue=\"payment_type\", data=df_final)\n","for p in ax.patches:\n","    ax.annotate('{:.1f}%'.format(100*p.get_height()/len(df_final)),(p.get_x()+0.05, p.get_height()+5))\n","ax.set_title('Review Score w.r.t payment method')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ji4yKlCP7W4C"},"source":["##### Observation\n","The plot above shows the distributio of the categorical variable payment type w.r.t the review score. From the plot we observe the around 55% of the positive review given by customers have used credit card for the payments. Similarly, for negative review around 18% of customers made payment using credit cared while in the second postion we have boleto -  digital currency provided by the eCommerce site for their regular and registered customers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zHYpmIhb7W4D"},"outputs":[],"source":["# count plot of order fullfillment\n","plt.figure(figsize=(8,5))\n","sns.set_style(\"whitegrid\")\n","ax = sns.countplot(x=\"review_score\", hue=\"order_status\", data=df_final)\n","ax.set_title('Review Score by order fullfillment')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ei1pmp5K7W4D"},"source":["##### Observation\n","The plot above is a very simple plot which shows the distribution of review score given per order status of the order. From the plot we can observe that out of all the orders which got positive review 99% of them has been successfully delivered."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9aehAi07W4D"},"outputs":[],"source":["# Top 10 shopping states\n","plt.figure(figsize=(8,5))\n","sns.set_style(\"whitegrid\")\n","ax = df_final.customer_state.value_counts().sort_values()[-10:].plot(kind='bar')\n","ax.set_title(\"Top ten consumer states of Brazil\")\n","ax.set_xlabel(\"States\")\n","plt.xticks(rotation=35)\n","ax.set_ylabel(\"Frequency\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S-nwC_Wn7W4E"},"outputs":[],"source":["# top 10 products categories from which products have been sold\n","plt.figure(figsize=(8,5))\n","sns.set_style(\"whitegrid\")\n","ax = df_final.product_category_name_english.value_counts().sort_values()[-10:].plot(kind='bar')\n","ax.set_title(\"Top ten product categories sold\")\n","ax.set_xlabel(\"Product category\")\n","plt.xticks(rotation=35)\n","ax.set_ylabel(\"Frequency\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"PX79eAyj7W4E"},"source":["##### Observation\n","In the above two plots titled `top 10 consumer states of Brazil` and `top 10 products categories sold` respectively we tried to observe the top ten states of the Brazil which shopped mostly online and the top ten product catefories from which products have been sold. In plot 1, we observe that around 45% of the consumers who shopped online is from the state **SP** while top 2 state consitute only around 15% of the total consumer shoppings of the data.\n","\n","From the second plot, we observe that most of the products sold is from category bed_bath_table. The top two products category health_beauty and bed_bath_table constitutes around 20% of the sells of the site."]},{"cell_type":"markdown","metadata":{"id":"T3UMgS8M7W4E"},"source":["### 3.4 Feature Engineering\n","let us create some features and analyse them.\n","1. **Sellers Count**:- Number of sellers for each product as a feature.\n","2. **Products count**:- Number of products ordered in each order as a feature.\n","3. **Estimated Delivery Time(in number of days)**:- Gets the days between order approval and estimated delivery date. A customer might be unsatisfied if he is told that the estimated time is big.\n","4. **Actual Delivery Time**:- Gets the days between order approval and delivered customer date. A customer might be more satisfied if he gets the product faster.\n","5. **Difference in delivery days**:- The difference between the actual and estimated date. If negative was delivered early, if positive was delivered late. A customer might be more satisfied if the order arrives sooner than expected, or unhappy if he receives after the deadline\n","6. **Is Late**:- Binary variable indicating if the order was delivered after the estimated date.\n","7. **Average Product Value**:- Cheaper products might have lower quality, leaving customers unhappy.\n","8. **Total Order Value**:- If a customer expends more, he might expect a better order fulfilment.\n","9. **Order Freight Ratio**:- If a customer pays more for freight, he might expect a better service.\n","10. **Purchase Day of Week**:- Day of week on which purchase was made.\n","11. **is_reviewed**:- If the review comment is given or not."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pSjJEuxa7W4F"},"outputs":[],"source":["# Finding number of sellers for each product as a feature\n","product_id = order_product_item_dataset.groupby('product_id').count()['seller_id'].index\n","seller_count = order_product_item_dataset.groupby('product_id').count()['seller_id'].values\n","product_seller_count = pd.DataFrame({'product_id':product_id,'sellers_count':seller_count})\n","product_seller_count.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ogbgG_B7W4F"},"outputs":[],"source":["# Finding number of products ordered in each order as a feature\n","order_id = order_product_item_dataset.groupby('order_id').count()['product_id'].index\n","pd_count = order_product_item_dataset.groupby('order_id').count()['product_id'].values\n","order_items_count = pd.DataFrame({'order_id':order_id,'products_count':pd_count})\n","order_items_count.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3H-3Y-Z7W4F"},"outputs":[],"source":["# Adding the seller count and products count feature to the final data set\n","df_final = pd.merge(df_final,product_seller_count,on='product_id')\n","df_final = pd.merge(df_final,order_items_count,on='order_id')"]},{"cell_type":"code","source":["# converting date to datetime and extracting dates from the datetime columns in the data set\n","datetime_cols = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n","\n","for col in datetime_cols:\n","    df_final[col] = pd.to_datetime(df_final[col])"],"metadata":{"id":"sRUoKMD5lwK0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLznP39b7W4G"},"outputs":[],"source":["# https://www.kaggle.com/andresionek/predicting-customer-satisfaction\n","# calculating estimated delivery time\n","df_final['estimated_delivery_time'] = (df_final['order_estimated_delivery_date'] - df_final['order_approved_at']).dt.days\n","\n","# calculating actual delivery time\n","df_final['actual_delivery_time'] = (df_final['order_delivered_customer_date'] - df_final['order_approved_at']).dt.days\n","\n","# calculating diff_in_delivery_time\n","df_final['diff_in_delivery_time'] = df_final['estimated_delivery_time'] - df_final['actual_delivery_time']\n","\n","# finding if delivery was lare\n","df_final['on_time_delivery'] = df_final['order_delivered_customer_date'] < df_final['order_estimated_delivery_date']\n","df_final['on_time_delivery'] = df_final['on_time_delivery'].astype('int')\n","\n","# calculating mean product value\n","df_final['avg_product_value'] = df_final['price']/df_final['products_count']\n","\n","# finding total order cost\n","df_final['total_order_cost'] = df_final['price'] + df_final['freight_value']\n","\n","# calculating order freight ratio\n","df_final['order_freight_ratio'] = df_final['freight_value']/df_final['price']\n","\n","# finding the day of week on which order was made\n","df_final['purchase_dayofweek'] = pd.to_datetime(df_final['order_purchase_timestamp']).dt.dayofweek\n","\n","# adding is_reviewed where 1 is if review comment is given otherwise 0.\n","df_final['is_reviewed'] = (df_final['review_comment_message'] != 'no_review').astype('int')"]},{"cell_type":"markdown","metadata":{"id":"vZT1f0Hl7W4G"},"source":["#### 3.4.1 Dropping date columns and id columns like seller_id, order_id etc."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcNm3WXP7W4G"},"outputs":[],"source":["df_final.drop(columns=['order_id', 'order_item_id', 'product_id', 'seller_id','shipping_limit_date','customer_id',\n","                       'order_purchase_timestamp', 'order_approved_at', 'order_delivered_customer_date', 'customer_state',\n","                       'order_estimated_delivery_date','customer_unique_id', 'customer_city','customer_zip_code_prefix'],\n","              axis=1,inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"fOA59Svm7W4H"},"outputs":[],"source":["# Final data set after feature creation and removing of irrelevant features\n","# df_final.to_csv('olist_final.csv',index=False)\n","df_final.head()"]},{"cell_type":"code","source":["df_final.info()"],"metadata":{"id":"PJ9BDN0el9I5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cqnUtw_57W4H"},"source":["#### 3.4.2 Analysis of the engineered features\n","#### 3.4.2.1 Understanding statistics of the engineered features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pAaNTIC7W4I"},"outputs":[],"source":["# describe the data set\n","df_final[['sellers_count', 'products_count','estimated_delivery_time', 'actual_delivery_time','diff_in_delivery_time',\n","          'avg_product_value','total_order_cost', 'order_freight_ratio']].describe()"]},{"cell_type":"markdown","metadata":{"id":"_vPfMt7O7W4I"},"source":["##### Observation\n","Let us observe the statistics of the features we created.\n","1. The sellers count for total products ordered in a order have minimum number of sellers as 1 while maximum sellers of 527.\n","2. The numer of products ordered in a single order have maximum value as 21 and minimum as 1.\n","3. The maximum estimated delivery time is 153 days with mean value of 23 days.\n","4. The maximum actual delivery time is 208 days and with average delivery time of 12 days.\n","5. Average total order cost 140 brazilian real with minimum value of 6 real."]},{"cell_type":"markdown","metadata":{"id":"0Nz7wUIr7W4I"},"source":["#### 3.4.3 Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kmmUcf8P7W4I"},"outputs":[],"source":["# distribution plot of actual delivery time\n","plt.figure()\n","sns.set_style(\"whitegrid\")\n","ax = sns.FacetGrid(df_final, hue=\"review_score\", height=5,aspect=2.0)\n","ax = ax.map(sns.distplot, \"actual_delivery_time\").add_legend();\n","plt.title('Distribution of payment_value per class')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEUMnYL47W4J"},"outputs":[],"source":["# distribution plot of payment value\n","plt.figure()\n","sns.set_style(\"whitegrid\")\n","ax = sns.FacetGrid(df_final, hue=\"review_score\", height=5,aspect=2.0)\n","ax = ax.map(sns.distplot, \"order_freight_ratio\").add_legend();\n","plt.title('Distribution of order_freight_ratio per class')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYhbqFBx7W4J"},"outputs":[],"source":["# distribution review by on time delivery\n","plt.figure(figsize=(8,5))\n","sns.set_style(\"whitegrid\")\n","ax = sns.countplot(x=\"review_score\", hue=\"on_time_delivery\", data=df_final)\n","ax.set_title('Review Score by timely delivery of orders')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Po1i5zAA7W4K"},"outputs":[],"source":["# Distribution of price vs freight_value per class\n","plt.figure(figsize=(8,5))\n","sns.set_style(\"whitegrid\")\n","ax = sns.scatterplot(x='estimated_delivery_time',y='actual_delivery_time', data = df_final, hue=\"review_score\")\n","plt.title('Distribution of estimated_delivery_time vs actual_delivery_time per class')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Irewf-Fu7W4K"},"outputs":[],"source":["# Distribution of price vs freight_value per class\n","plt.figure(figsize=(8,5))\n","sns.set_style(\"whitegrid\")\n","ax = sns.scatterplot(x='sellers_count',y='actual_delivery_time', data = df_final, hue=\"review_score\")\n","plt.title('Distribution of sellers_count vs actual_delivery_time per class')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"p4_W9kiL7W4L"},"source":["###### Observation\n","From the univariate analysis, we have seen that actual delivery time's distibution is partially overlapping for both the class lables and we cannot derive any certain rule to classify them based on the actual delivery time. So, we plotted the above scatter plots to see if we can derive any relation with more than one features. From the plots above we observe that we can separate the blue points from the orange points with a linear line with some errors. Thus, now we can say that we can derive some linear relation to classify them. let us verify our observation from below pair plot."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDTa33ON7W4L"},"outputs":[],"source":["# pair plot\n","feat = ['estimated_delivery_time','actual_delivery_time', 'diff_in_delivery_time','avg_product_value',\n","           'total_order_cost', 'order_freight_ratio','products_count','sellers_count','review_score']\n","sns.set(style=\"ticks\", color_codes=True)\n","pp = sns.pairplot(df_final[feat],hue='review_score')\n","# pp.savefig(\"pairplot2.png\")"]},{"cell_type":"markdown","metadata":{"id":"N2sLO_Vd7W4L"},"source":["###### Observation\n","The Pairplot above is the bivariate analysis of 8 newly engineered features like sellers count, product count, delivery time in days etc. From the above plot we observe that actual delivery time vs sellers count plot separates the positive and negative classed more visibly than others. We can say that the number of sellers available and the actual delivery time affects the review score either postively or negatively which we can see in details in the correlation analysis of the features."]},{"cell_type":"markdown","metadata":{"id":"1cj0kTdE7W4L"},"source":["## 4. Observation on EDA and FE\n","Let us gather what we have observed and learned so far from the data.\n","1. The class label is not balanced.\n","2. It is impossible to differentiate the classes based on any single feature.\n","3. The numerical feature like price and freight value have skewed distribution suggesting the presence of high boundary values.\n","4. The freight_value vs product photo qty plot shows good result for classifying the class labels.\n","5. The most used payment method is credit card.\n","6. We also found that aroudn 45% of the consumers belong to single state and most shopped product category among them is bed,bath,table,health and beauty.\n","7. As observed in the analysis, 10 different features were  created referencing some features in the data like:- products ccount, sellers count, total order cost, actual delivery time etc. and the previous features like delivery date, product_id etc. is dropped.\n","8. we learnt that the feature actual delivery time provide partial differentiation between positive and negative class.\n","9. The scatter plots plotted in the first part of thge analysis do not show any significant results in classifying the positive and negative points while the scatter plots in the second analysis after feature engineering shows significant difference in the positive and negative class points as evident from plot `Distribution of sellers_count vs actual_delivery_time per class`.\n","10. The pair plot shows that with these features can classify the postive class from negative class with some non linear transformation.\n","11. As evident from scatter plots and pair plots , it is clear that linear algorithms or KNN might not be good choice in classifying these points. But, atleast we learnt that with thes set of features we can classify them into two classes."]},{"cell_type":"markdown","metadata":{"id":"DJC-4RrT7W4M"},"source":["## 5. Data Preparation\n","### 5.1 Getting Numerical and categorical features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TFzJkB4X7W4M"},"outputs":[],"source":["# selecting features\n","# numerical features\n","num_feat = ['price', 'freight_value', 'product_name_lenght','product_description_lenght', 'product_photos_qty',\n","           'product_weight_g','product_length_cm', 'product_height_cm', 'product_width_cm','sellers_count',\n","           'products_count', 'payment_sequential','payment_installments', 'payment_value','on_time_delivery',\n","           'estimated_delivery_time','actual_delivery_time', 'diff_in_delivery_time','avg_product_value', 'purchase_dayofweek',\n","           'total_order_cost', 'order_freight_ratio','is_reviewed']\n","\n","# categorical features\n","cat_feat = ['review_comment_message','product_category_name_english','order_status', 'payment_type']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"alndXalm7W4M"},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","si = SimpleImputer(strategy='median')\n","si.fit(df_final[num_feat])\n","df_final[num_feat] = si.transform(df_final[num_feat])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVBCiyvc7W4N"},"outputs":[],"source":["# checking values of categorical features\n","print(\"order Status: \",df_final.order_status.value_counts())\n","print(\"----------------------------------------------------------------------------\")\n","print(\"Payment type: \",df_final.payment_type.value_counts())"]},{"cell_type":"markdown","metadata":{"id":"UzEM_6cO7W4N"},"source":["We see that the we have two types and 3 types of data in on_time_delivery and payment_type column in the data. so, we will go with label encoding for these data rather than one hot encoding. Let us look at the review comments feature."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdEfpQoR7W4N"},"outputs":[],"source":["df_final['review_comment_message'][:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"voJNxw1y7W4N"},"outputs":[],"source":["# https://www.aclweb.org/anthology/W17-6615\n","\n","def process_data(texts):\n","\n","    processed_text = []\n","\n","    portuguese_stopwords = stopwords.words('portuguese') # portugese language stopwords\n","    stemmer = RSLPStemmer() # portugese language stemmer\n","\n","    links = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+' # check for hyperlinks\n","    dates = '([0-2][0-9]|(3)[0-1])(\\/|\\.)(((0)[0-9])|((1)[0-2]))(\\/|\\.)\\d{2,4}' # check for dates\n","    currency = '[R]{0,1}\\$[ ]{0,}\\d+(,|\\.)\\d+' # check for currency symbols\n","\n","    for text in tqdm(texts):\n","        text = re.sub('[\\n\\r]', ' ', text) # remove new lines\n","        text = re.sub(links, ' URL ', text) # remove hyperlinks\n","        text = re.sub(dates, ' ', text) # remove dates\n","        text = re.sub(currency, ' dinheiro ', text) # remove currency symbols\n","        text = re.sub('[0-9]+', ' numero ', text) # remove digits\n","        text = re.sub('([nN][ãÃaA][oO]|[ñÑ]| [nN] )', ' negação ', text) # replace no with negative\n","        text = re.sub('\\W', ' ', text) # remove extra whitespaces\n","        text = re.sub('\\s+', ' ', text) # remove extra spaces\n","        text = re.sub('[ \\t]+$', '', text) # remove tabs etc.\n","        text = ' '.join(e for e in text.split() if e.lower() not in portuguese_stopwords) # remove stopwords\n","#         text = ' '.join(stemmer.stem(e.lower()) for e in text.split()) # stemming the words\n","        processed_text.append(text.lower().strip())\n","\n","    return processed_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBERCYkg7W4P"},"outputs":[],"source":["processed_text = process_data(df_final['review_comment_message'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CjdCMzfn7W4P"},"outputs":[],"source":["df_final['review_comment_message'] = processed_text\n","# nao_reveja = no_review in portugese\n","df_final['review_comment_message'] = df_final['review_comment_message'].replace({'no_review':'nao_reveja'})\n","# df_final.to_csv('olist_final.csv',index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bh3d5Tt_7W4P"},"outputs":[],"source":["df_final['review_comment_message'].iloc[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZrbQbke7W4Q"},"outputs":[],"source":["# Encoding categorical variable\n","df_final['payment_type'] = df_final['payment_type'].replace({'credit_card':1,'boleto':2,'voucher':3,'debit_card':4})"]},{"cell_type":"markdown","metadata":{"id":"qjiIGG7b7W4Q"},"source":["#### 4.2 Splitting data into test and train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8drgtXFb7W4R"},"outputs":[],"source":["# separating the target variable\n","y = df_final['review_score']\n","X = df_final.drop(labels='review_score',axis=1)\n","\n","# train test 80:20 split\n","X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=25)\n","print(\"Train data: \",X_train.shape,y_train.shape)\n","print(\"Train data: \",X_test.shape,y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"oxUzYriB7W4R"},"source":["#### 4.3 Encoding categorical features\n","#### 4.3.1 Encoding categorical feature order_status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IyAf6TAM7W4R"},"outputs":[],"source":["# encoding feature order status\n","vect = CountVectorizer()\n","vect.fit(X_train['order_status'])\n","training_os = vect.transform(X_train['order_status'])\n","test_os = vect.transform(X_test['order_status'])\n","\n","\n","print(\"training product category: \",training_os.shape)\n","print(\"test product category: \",test_os.shape)"]},{"cell_type":"markdown","metadata":{"id":"MnTeKrt-7W4R"},"source":["#### 4.3.2 Encoding categorical feature product category"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQaS1vTS7W4S"},"outputs":[],"source":["# encoding product category\n","cv = CountVectorizer()\n","cv.fit(X_train['product_category_name_english'])\n","training_pc = cv.transform(X_train['product_category_name_english'])\n","test_pc = cv.transform(X_test['product_category_name_english'])\n","\n","print(\"training product category: \",training_pc.shape)\n","print(\"test product category: \",test_pc.shape)"]},{"cell_type":"markdown","metadata":{"id":"yCH3iVRQ7W4S"},"source":["#### 4.3.2 Encoding categorical feature review_comment_message"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJfDqeSL7W4S"},"outputs":[],"source":["# # Word2vec encoding\n","# # https://radimrehurek.com/gensim/models/word2vec.html\n","from gensim.test.utils import common_texts, get_tmpfile\n","from gensim.models import Word2Vec\n","\n","path = get_tmpfile(\"word2vec.model\")\n","\n","texts = [x.split(' ') for x in df_final['review_comment_message']]\n","\n","w2vmodel = Word2Vec(texts, vector_size=300, window=5, min_count=1, workers=4)\n","w2vmodel.save(\"word2vec.model\")\n","\n","w2vmodel = Word2Vec.load(\"word2vec.model\")\n","\n","initial_alpha = 0.01\n","min_alpha_value = 0.0001\n","\n","texts = [x.split(' ') for x in df_final['review_comment_message']]\n","w2vmodel.train(texts, total_examples=len(texts), epochs=10)\n","\n","w2vmodel.wv['nao_reveja'].shape  # numpy vector of a word"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nO3UpzVV7W4T"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","# Retrieve the list of words from the Word2Vec model's vocabulary\n","words = list(w2vmodel.wv.index_to_key)\n","# Retrieve vectors for each word\n","vectors = [w2vmodel.wv[word] for word in words]\n","\n","# Initialize TSNE\n","tsne = TSNE(n_components=2, random_state=0)\n","\n","# Limit the number of words and vectors for visualization\n","limited_words = words[:100]\n","limited_vectors = np.array(vectors[:100])  # Convert list to numpy array\n","\n","# Apply TSNE\n","Y = tsne.fit_transform(limited_vectors)\n","\n","# Plot the results using matplotlib\n","plt.scatter(Y[:, 0], Y[:, 1])\n","for label, x, y in zip(limited_words, Y[:, 0], Y[:, 1]):\n","    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords=\"offset points\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"bTUFt9-u7W4T"},"outputs":[],"source":["# http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc#\n","# Reading glove vectors in python: https://stackoverflow.com/a/38230349/4084039\n","def loadGloveModel(gloveFile):\n","    print (\"Loading Glove Model\")\n","    f = open(gloveFile,'r', encoding=\"utf8\")\n","    model = {}\n","    for line in tqdm(f.readlines()[1:]):\n","        splitLine = line.split(' ')\n","        word = splitLine[0]\n","        embedding = np.asarray(splitLine[1:], \"float32\")#np.array([float(0) if val=='0,0' else float(val) for val in splitLine[1:]])\n","        model[word] = embedding\n","    print (\"Done.\",len(model),\" words loaded!\")\n","    return model\n","\n","embeddings = loadGloveModel('/content/drive/MyDrive/Predict Customer Satisfaction /Trang /glove_s300.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N4t9qyf27W4T"},"outputs":[],"source":["def tfidfWord2Vector(text,glove_words,tfidf_words,tf_values):\n","    # average Word2Vec\n","    # compute average word2vec for each review.\n","    tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n","    for sentence in tqdm(text): # for each review/sentence\n","        vector = np.zeros(300) # as word vectors are of zero length\n","        tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n","        for word in sentence.split(): # for each word in a review/sentence\n","            if (word in glove_words) and (word in tfidf_words):\n","                vec = w2vmodel.wv[word] # embeddings[word]\n","                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n","                tf_idf = tf_values[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n","                vector += (vec * tf_idf) # calculating tfidf weighted w2v\n","                tf_idf_weight += tf_idf\n","        if tf_idf_weight != 0:\n","            vector /= tf_idf_weight\n","        tfidf_w2v_vectors.append(vector)\n","    tfidf_w2v_vectors = np.asarray(tfidf_w2v_vectors)\n","\n","    return tfidf_w2v_vectors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqamgh3I7W4U"},"outputs":[],"source":["# encoding review comment message using Tfidf weighted W2V\n","tfidf = TfidfVectorizer()\n","tfidf.fit(X_train['review_comment_message'])\n","# pickle.dump(tfidf,open('tfidf_review_comments.pkl','wb'))\n","\n","# we are converting a dictionary with word as a key, and the idf as a value\n","tf_values = dict(zip(tfidf.get_feature_names_out(), list(tfidf.idf_)))\n","tfidf_words = set(tfidf.get_feature_names_out())\n","glove_words = list(w2vmodel.wv.index_to_key) # list(embeddings.keys())\n","\n","tfidf_w2v_vectors_train = tfidfWord2Vector(X_train['review_comment_message'].values,glove_words,tfidf_words,tf_values)\n","tfidf_w2v_vectors_test = tfidfWord2Vector(X_test['review_comment_message'].values,glove_words,tfidf_words,tf_values)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sVhC3oGl7W4V"},"outputs":[],"source":["tfidf_w2v_vectors_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vn6Iil7O7W4V"},"outputs":[],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","vocab = list()\n","for x in df_final['review_comment_message']:\n","    vocab.extend(x.split())\n","vocab = set(vocab)\n","word_index = {word:i+1 for i,word in enumerate(vocab)}\n","# pickle.dump(word_index,open('word_index.pkl','wb'))\n","vocab_size = len(word_index)+1\n","# integer encode the documents\n","X_train_encoded_text = []\n","for x in X_train['review_comment_message']:\n","    X_train_encoded_text.append([word_index[w] for w in x.split()])\n","\n","X_test_encoded_text = []\n","for y in X_test['review_comment_message']:\n","    X_test_encoded_text.append([word_index[w] for w in y.split()])\n","\n","\n","# pad documents to a max length of 122 words as 95 percentile is 122\n","max_length = 122\n","X_train_padded_text = pad_sequences(X_train_encoded_text, maxlen=max_length, padding='post')\n","X_test_padded_text = pad_sequences(X_test_encoded_text, maxlen=max_length, padding='post')\n","\n","\n","print(X_train_padded_text.shape,X_test_padded_text.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_xfReM-7W4V"},"outputs":[],"source":["# creating embedding matrix\n","embedding_matrix = np.zeros((vocab_size, 300))\n","for word,i in word_index.items():\n","    embedding_vector = w2vmodel.wv[word]\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","print(embedding_matrix.shape)\n","# pickle.dump(embedding_matrix,open('embedding_matrix.pkl','wb'))"]},{"cell_type":"markdown","metadata":{"id":"ADEyUDjT7W4W"},"source":["#### 4.4 Encoding numerical features"]},{"cell_type":"code","source":["normalizer = Normalizer()\n","\n","X_train[num_feat] = normalizer.fit_transform(X_train[num_feat])\n","X_test[num_feat] = normalizer.transform(X_test[num_feat])"],"metadata":{"id":"JXetLRKJ5V5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dIY4qVt7W4W"},"outputs":[],"source":["# dropping categorical features\n","\n","X_train = X_train.drop(labels=['review_comment_message','product_category_name_english','order_status'],axis=1)\n","X_test = X_test.drop(labels=['review_comment_message','product_category_name_english','order_status'],axis=1)\n","\n","print(X_train.shape,X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"CBqcHCc17W4X"},"source":["#### 4.5 Merging all the features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujGKyIeM7W4X"},"outputs":[],"source":["# merging our encoded categorical features with rest of the data\n","X_train_merge = hstack((X_train, training_pc, training_os, tfidf_w2v_vectors_train))\n","X_test_merge = hstack((X_test, test_pc, test_os, tfidf_w2v_vectors_test))\n","\n","print(\"Train shape:\",X_train_merge.shape)\n","print(\"Test shape:\",X_test_merge.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkek40at7W4X"},"outputs":[],"source":["# merging our encoded categorical features with rest of the data\n","X_train_other = hstack((X_train, training_pc, training_os))\n","X_test_other = hstack((X_test, test_pc, test_os))\n","\n","print(\"Train shape:\",X_train_other.shape)\n","print(\"Test shape:\",X_test_other.shape)"]},{"cell_type":"markdown","metadata":{"id":"1ostqmDJ7W4X"},"source":["## 6. Model Selection\n","### 6.1 Logistic Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hOZDKjsl7W4Y"},"outputs":[],"source":["def confusion_matrices_plot(y_real, y_pred, y_test,y_test_pred):\n","    # representing confusion matric in heatmap format\n","    # https://seaborn.pydata.org/generated/seaborn.heatmap.html\n","    cmap=sns.light_palette(\"brown\")\n","    C1 = confusion_matrix(y_real,y_pred)\n","    C2 = confusion_matrix(y_test,y_test_pred)\n","\n","    fig,ax = plt.subplots(1, 2, figsize=(15,5))\n","    ax1 = sns.heatmap(C1, annot=True, cmap=cmap, fmt=\".2f\", ax = ax[0])\n","    ax1.set_xlabel('Predicted Class')\n","    ax1.set_ylabel('Original Class')\n","    ax1.set_title(\"Train Confusion matrix\")\n","\n","    ax2 = sns.heatmap(C2, annot=True, cmap=cmap, fmt=\".2f\", ax = ax[1])\n","    ax2.set_xlabel('Predicted Class')\n","    ax2.set_ylabel('Original Class')\n","    ax2.set_title(\"Test Confusion matrix\")\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvTugMoy7W4Y"},"outputs":[],"source":["# Training Logistic regression model and chekcing f1 score metric\n","alpha = [0.001,0.01,0.1,1,10,100,1000]\n","train_scores = [] # store train scores\n","test_scores = [] # store test scores\n","\n","for i in alpha:\n","    lr = SGDClassifier(loss='log', penalty='l2', alpha=i, n_jobs=-1, random_state=25)\n","    lr.fit(X_train_merge,y_train)\n","    train_sc = f1_score(y_train,lr.predict(X_train_merge))\n","    test_sc = f1_score(y_test,lr.predict(X_test_merge))\n","    test_scores.append(test_sc)\n","    train_scores.append(train_sc)\n","    print('Alpha = ',i,'Train Score',train_sc,'test Score',test_sc)\n","\n","# plotting the scores vs parameters\n","plt.plot(np.log(alpha),train_scores,label='Train Score')\n","plt.plot(np.log(alpha),test_scores,label='Test Score')\n","plt.xlabel('Alpha')\n","plt.ylabel('Score')\n","plt.title('Alpha vs Score')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYQJMA8H7W4Y"},"outputs":[],"source":["# Parameter tuning of Logistic regression using RandomisedSearch CV technique\n","sgd = SGDClassifier(loss='log', n_jobs=-1, random_state=25)\n","\n","prams={ 'alpha': [0.001,0.01,0.1,1,10,100,1000] }\n","\n","random_cfl1 = RandomizedSearchCV(sgd,param_distributions=prams,verbose=10,scoring='f1',n_jobs=-1,random_state=25,\n","                               return_train_score=True)\n","random_cfl1.fit(X_train_merge,y_train)\n","\n","print('mean test scores',random_cfl1.cv_results_['mean_test_score'])\n","print('mean train scores',random_cfl1.cv_results_['mean_train_score'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DC-0uCeS7W4Z"},"outputs":[],"source":["# printing best parameters and score\n","print(\"Best Parameters: \",random_cfl1.best_params_)\n","print(\"Best Score: \",random_cfl1.best_score_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WPL6bUOf7W4Z"},"outputs":[],"source":["# Fitting LogisticRegression mpdel on best parameters\n","sgd = SGDClassifier(loss='log', alpha=0.001, n_jobs=-1, random_state=25)\n","sgd.fit(X_train_merge,y_train)\n","if not os.path.exists('models'):\n","    os.makedirs('models')\n","\n","with open('models/logistic.pkl', 'wb') as file:\n","    pickle.dump(sgd, file)\n","\n","y_train_pred = sgd.predict(X_train_merge)\n","y_test_pred = sgd.predict(X_test_merge)\n","\n","# printing train and test scores\n","print('Train f1 score: ',f1_score(y_train,y_train_pred))\n","print('Test f1 score: ',f1_score(y_test,y_test_pred))"]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score\n","\n","def print_evaluation_scores(y_true, y_pred, y_proba, set_name=\"Set\"):\n","    accuracy = accuracy_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    auc = roc_auc_score(y_true, y_proba)\n","\n","    print(f\"{set_name} Accuracy: {accuracy:.4f}\")\n","    print(f\"{set_name} F1 Score: {f1:.4f}\")\n","    print(f\"{set_name} Recall: {recall:.4f}\")\n","    print(f\"{set_name} Precision: {precision:.4f}\")\n","    print(f\"{set_name} AUC: {auc:.4f}\\n\")"],"metadata":{"id":"BXqNj3yLBYWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_test_proba = sgd.predict_proba(X_test_merge)[:, 1]\n","print_evaluation_scores(y_test, y_test_pred, y_test_proba, \"Test\")"],"metadata":{"id":"8kfrsK48ETlr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPeqILmV7W4Z"},"outputs":[],"source":["confusion_matrices_plot(y_train,y_train_pred,y_test,y_test_pred)"]},{"cell_type":"markdown","metadata":{"id":"D27AFjNN7W4a"},"source":["### 6.2 Linear SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O43H7Fcg7W4a"},"outputs":[],"source":["# Training Logistic regression model and chekcing f1 score metric\n","alpha = [0.001,0.01,0.1,1,10,100,1000]\n","train_scores = [] # store train scores\n","test_scores = [] # store test scores\n","\n","for i in alpha:\n","    lr = SGDClassifier(loss='log', penalty='l2', alpha=i, n_jobs=-1, random_state=25)\n","    lr.fit(X_train_merge,y_train)\n","    train_sc = f1_score(y_train,lr.predict(X_train_merge))\n","    test_sc = f1_score(y_test,lr.predict(X_test_merge))\n","    test_scores.append(test_sc)\n","    train_scores.append(train_sc)\n","    print('Alpha = ',i,'Train Score',train_sc,'test Score',test_sc)\n","\n","# plotting the scores vs parameters\n","plt.plot(np.log(alpha),train_scores,label='Train Score')\n","plt.plot(np.log(alpha),test_scores,label='Test Score')\n","plt.xlabel('Alpha')\n","plt.ylabel('Score')\n","plt.title('Alpha vs Score')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"075x0ULY7W4a"},"outputs":[],"source":["# Parameter tuning of Logistic regression using RandomisedSearch CV technique\n","sgd = SGDClassifier(loss='log', n_jobs=-1, random_state=25)\n","\n","prams={ 'alpha': [0.001,0.01,0.1,1,10,100,1000] }\n","\n","random_cfl1 = RandomizedSearchCV(sgd,param_distributions=prams,verbose=10,scoring='f1',n_jobs=-1,random_state=25,\n","                               return_train_score=True)\n","random_cfl1.fit(X_train_merge,y_train)\n","\n","print('mean test scores',random_cfl1.cv_results_['mean_test_score'])\n","print('mean train scores',random_cfl1.cv_results_['mean_train_score'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WnRAbW_V7W4b"},"outputs":[],"source":["# printing best parameters and score\n","print(\"Best Parameters: \",random_cfl1.best_params_)\n","print(\"Best Score: \",random_cfl1.best_score_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OSpiRkW47W4b"},"outputs":[],"source":["# Fitting LogisticRegression mpdel on best parameters\n","sgd = SGDClassifier(loss='log',alpha=0.001, n_jobs=-1, random_state=25)\n","sgd.fit(X_train_merge,y_train)\n","pickle.dump(sgd,open('models/svm.pkl','wb'))\n","\n","y_train_pred = sgd.predict(X_train_merge)\n","y_test_pred = sgd.predict(X_test_merge)\n","\n","# printing train and test scores\n","print('Train f1 score: ',f1_score(y_train,y_train_pred))\n","print('Test f1 score: ',f1_score(y_test,y_test_pred))"]},{"cell_type":"code","source":["y_test_proba = sgd.predict_proba(X_test_merge)[:, 1]\n","print_evaluation_scores(y_test, y_test_pred, y_test_proba, \"Test\")"],"metadata":{"id":"3CrwQag-E2qm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XZ-tJuId7W4b"},"outputs":[],"source":["confusion_matrices_plot(y_train,y_train_pred,y_test,y_test_pred)"]},{"cell_type":"markdown","metadata":{"id":"D9bGvX-m7W4c"},"source":["### 6.3 Decision Tree"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CW5zLv8m7W4c"},"outputs":[],"source":["# Checking the variation of score with depth parameters of Decision Tree\n","depth = [3,10,50,100,250,500]\n","train_scores = []\n","test_scores = []\n","for i in depth:\n","    clf = DecisionTreeClassifier(max_depth=i,random_state=25)\n","    clf.fit(X_train_merge,y_train)\n","    train_sc = f1_score(y_train,clf.predict(X_train_merge))\n","    test_sc = f1_score(y_test,clf.predict(X_test_merge))\n","    test_scores.append(test_sc)\n","    train_scores.append(train_sc)\n","    print('Depth = ',i,'Train Score',train_sc,'test Score',test_sc)\n","\n","# plotting the score vs depth\n","plt.plot(depth,train_scores,label='Train Score')\n","plt.plot(depth,test_scores,label='Test Score')\n","plt.xlabel('Depth')\n","plt.ylabel('Score')\n","plt.title('Depth vs Score')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CK0lU-Md7W4d"},"outputs":[],"source":["# Parameter tuning of DecisionTreeClassifier using RandomisedSearch CV technique\n","# https://medium.com/@mohtedibf/indepth-parameter-tuning-for-decision-tree-6753118a03c3\n","dt = DecisionTreeClassifier(random_state=25)\n","\n","params = { \"max_depth\": sp_randint(3,500), \"min_samples_split\": sp_randint(50,200), \"min_samples_leaf\": sp_randint(2,50)}\n","\n","random_cfl1 = RandomizedSearchCV(dt, param_distributions=params,verbose=10,scoring='f1',n_jobs=-1,random_state=25,\n","                               return_train_score=True)\n","random_cfl1.fit(X_train_merge,y_train)\n","\n","print('mean test scores',random_cfl1.cv_results_['mean_test_score'])\n","print('mean train scores',random_cfl1.cv_results_['mean_train_score'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J67PzoKU7W4d"},"outputs":[],"source":["# printing best parameters and scores\n","print(\"Best Parameters: \",random_cfl1.best_params_)\n","print(\"Best Score: \",random_cfl1.best_score_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DW9q4_ae7W4d"},"outputs":[],"source":["# Fitting the model on best parameters\n","dt = DecisionTreeClassifier(max_depth = 320, min_samples_leaf = 25, min_samples_split = 186,random_state=25)\n","dt.fit(X_train_merge,y_train)\n","pickle.dump(dt,open('models/decision_tree.pkl','wb'))\n","\n","y_train_pred = dt.predict(X_train_merge)\n","y_test_pred = dt.predict(X_test_merge)\n","\n","# printing train test score\n","print('Train f1 score',f1_score(y_train,y_train_pred))\n","print('Test f1 score',f1_score(y_test,y_test_pred))"]},{"cell_type":"code","source":["y_test_proba = sgd.predict_proba(X_test_merge)[:, 1]\n","print_evaluation_scores(y_test, y_test_pred, y_test_proba, \"Test\")"],"metadata":{"id":"t1dNVKadOT8e"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fE13pZds7W4e"},"outputs":[],"source":["confusion_matrices_plot(y_train,y_train_pred,y_test,y_test_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YNTah28K7W4e"},"outputs":[],"source":["# checking some top features\n","features = list(X_train.columns.values) + cv.get_feature_names_out() + vect.get_feature_names_out() + tf.get_feature_names_out()\n","importances = dt.feature_importances_ # importance extracted from the model\n","indices = np.argsort(importances)[-21:] # top 20 features by importance value\n","\n","# plotting top 20 features\n","plt.figure(figsize=(10,5))\n","plt.title('Feature Importances')\n","plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"q7QKjiuA7W4e"},"source":["###### Observation\n","After not getting promising results with linear models, we go with non linear models i.e tree and used DecsionTree Classifier as classification model. Firstly we trained the model for various values of depth of the tree to get the range of the parameters while fine uning the parameters during randomised serach cross validation. Keeping the results in last step as base line, the score obtained by this model during training is 0.88 which is better than previous model.\n","\n","Also, looking at the confusion matrices we see that we made some improvement in FalsePositives but while lost some in FalseNegatives suggesting that model is obviously learning differently than linear models. So, keeping these results in mind let us try ensemble techniques to further verify our hypothesis and try to achieve improved results.\n","\n","We have also plotted feature importance map to look at some of the top features."]},{"cell_type":"markdown","metadata":{"id":"PP4Aw3l77W4f"},"source":["### 6.4 Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIeXN8J87W4f"},"outputs":[],"source":["# Variation of score with estimators used in Random forest with other parameters set to constant value\n","estimators = [1,2,5,10,50,100,250,500]\n","train_scores = []\n","test_scores = []\n","for i in estimators:\n","    clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n","            max_depth=5, max_features='auto', max_leaf_nodes=None,\n","            min_impurity_decrease=0.0, min_impurity_split=None,\n","            min_samples_leaf=52, min_samples_split=120,\n","            min_weight_fraction_leaf=0.0, n_estimators=i, n_jobs=-1,random_state=25,verbose=0,warm_start=False)\n","    clf.fit(X_train_merge,y_train)\n","    train_sc = f1_score(y_train,clf.predict(X_train_merge))\n","    test_sc = f1_score(y_test,clf.predict(X_test_merge))\n","    test_scores.append(test_sc)\n","    train_scores.append(train_sc)\n","    print('Estimators = ',i,'Train Score',train_sc,'test Score',test_sc)\n","plt.plot(estimators,train_scores,label='Train Score')\n","plt.plot(estimators,test_scores,label='Test Score')\n","plt.xlabel('Estimators')\n","plt.ylabel('Score')\n","plt.title('Estimators vs score at depth of 5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1IovJ5O7W4f"},"outputs":[],"source":["# Parameter tuning of Random forest classifier using Randomised search CV\n","param_dist = {\"n_estimators\":sp_randint(1,500),\n","              \"max_depth\": sp_randint(3,20),\n","              \"min_samples_split\": sp_randint(50,200),\n","              \"min_samples_leaf\": sp_randint(2,50)}\n","\n","clf = RandomForestClassifier(random_state=25,n_jobs=-1)\n","\n","random_cfl1 = RandomizedSearchCV(clf,param_distributions=param_dist,scoring='f1',verbose=10,n_jobs=-1,random_state=25,\n","                               return_train_score=True)\n","random_cfl1.fit(X_train_merge,y_train)\n","\n","print('mean test scores',random_cfl1.cv_results_['mean_test_score'])\n","print('mean train scores',random_cfl1.cv_results_['mean_train_score'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p5LPgW9l7W4f"},"outputs":[],"source":["# printing best parameters and score\n","print(\"Best Parameters: \",random_cfl1.best_params_)\n","print(\"Best Score: \",random_cfl1.best_score_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ya3wgozm7W4g"},"outputs":[],"source":["# Fitting the model on best parameters\n","rf = RandomForestClassifier(max_depth = 19, min_samples_leaf = 40, min_samples_split = 166, n_estimators = 131,random_state=25,\n","                           n_jobs=-1)\n","rf.fit(X_train_merge,y_train)\n","pickle.dump(rf,open('models/random_forest.pkl','wb'))\n","\n","y_train_pred = rf.predict(X_train_merge)\n","y_test_pred = rf.predict(X_test_merge)\n","\n","# printing train and test scores\n","print('Train f1 score',f1_score(y_train,y_train_pred))\n","print('Test f1 score',f1_score(y_test,y_test_pred))"]},{"cell_type":"code","source":["y_test_proba = sgd.predict_proba(X_test_merge)[:, 1]\n","print_evaluation_scores(y_test, y_test_pred, y_test_proba, \"Test\")"],"metadata":{"id":"bWHiv2ampWVK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XpnA-_Cz7W4g"},"outputs":[],"source":["confusion_matrices_plot(y_train,y_train_pred,y_test,y_test_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufxl6U_T7W4g"},"outputs":[],"source":["# plotting top 25 features\n","# features = list(X_train.columns.values) + cv.get_feature_names() + vect.get_feature_names() # features list\n","importances = rf.feature_importances_ # importance generated by model\n","indices = np.argsort(importances)[-25:]\n","\n","plt.figure(figsize=(10,5))\n","plt.title('Feature Importances')\n","plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"UvKhEhLL7W4h"},"source":["### 6.5 LightGBM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MmNdTBrA7W4i"},"outputs":[],"source":["# Variation of score with estimators used in LGBM with other parameters set to default value\n","estimators = [1,3,5,10,50,100,250,500,1000]\n","train_scores = []\n","test_scores = []\n","for i in estimators:\n","    clf = LGBMClassifier(n_estimators=i, n_jobs=-1,random_state=25)\n","    clf.fit(X_train_merge,y_train)\n","    train_sc = f1_score(y_train,clf.predict(X_train_merge))\n","    test_sc = f1_score(y_test,clf.predict(X_test_merge))\n","    test_scores.append(test_sc)\n","    train_scores.append(train_sc)\n","    print('Estimators = ',i,'Train Score',train_sc,'test Score',test_sc)\n","plt.plot(estimators,train_scores,label='Train Score')\n","plt.plot(estimators,test_scores,label='Test Score')\n","plt.xlabel('Estimators')\n","plt.ylabel('Score')\n","plt.title('Estimators vs score at depth of 5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9GOfwq87W4i"},"outputs":[],"source":["# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n","# Prameter tuning of the LGBM parameters using RandonSearch CV\n","x_cfl=LGBMClassifier(random_state=25,n_jobs=-1)\n","\n","prams={\n","    'learning_rate':[0.001,0.01,0.03,0.05,0.1,0.15,0.2],\n","     'n_estimators':[1,3,5,10,50,100,250,500,1000],\n","     'max_depth':[3,5,10,15,20,50],\n","    'colsample_bytree':[0.1,0.3,0.5,1],\n","    'subsample':[0.1,0.3,0.5,1]\n","}\n","random_cfl1=RandomizedSearchCV(x_cfl,param_distributions=prams,verbose=10,n_jobs=-1,random_state=25,scoring='f1',\n","                               return_train_score=True)\n","random_cfl1.fit(X_train_merge,y_train)\n","\n","print('mean test scores',random_cfl1.cv_results_['mean_test_score'])\n","print('mean train scores',random_cfl1.cv_results_['mean_train_score'])"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"8ij_A9mW7W4k"},"outputs":[],"source":["# printing best parameters and score\n","print(\"Best Parameters: \",random_cfl1.best_params_)\n","print(\"Best Score: \",random_cfl1.best_score_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5DPyhW67W4l"},"outputs":[],"source":["# Fitting the model on best parameters\n","lgbm = LGBMClassifier(n_estimators=1000, max_depth=5,subsample=0.5,learning_rate=0.05,colsample_bytree=1,random_state=25,\n","                      n_jobs=-1)\n","lgbm.fit(X_train_merge,y_train)\n","pickle.dump(lgbm,open('models/lgbm.pkl','wb'))\n","\n","y_train_pred = lgbm.predict(X_train_merge)\n","y_test_pred = lgbm.predict(X_test_merge)\n","\n","# printing train and test scores\n","print('Train f1 score',f1_score(y_train,y_train_pred))\n","print('Test f1 score',f1_score(y_test,y_test_pred))"]},{"cell_type":"code","source":["y_test_proba = sgd.predict_proba(X_test_merge)[:, 1]\n","print_evaluation_scores(y_test, y_test_pred, y_test_proba, \"Test\")"],"metadata":{"id":"-ZQfP6ORpZOg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQMacGeg7W4m"},"outputs":[],"source":["confusion_matrices_plot(y_train,y_train_pred,y_test,y_test_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njRfQ08z7W4n"},"outputs":[],"source":["# ovserving top 25 features\n","importances = lgbm.feature_importances_\n","indices = np.argsort(importances)[-25:]\n","\n","plt.figure(figsize=(10,5))\n","plt.title('Feature Importances')\n","plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n","plt.yticks(range(len(indices)), [features[i] for i in indices])\n","plt.xlabel('Relative Importance')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"1trbin7G7W4o"},"source":["### XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"klmX-Dwb7W4o"},"outputs":[],"source":["# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n","# Prameter tuning of the LGBM parameters using RandonSearch CV\n","x_cfl=XGBClassifier(random_state=25,n_jobs=-1)\n","\n","prams={\n","    'learning_rate':[0.001,0.01,0.03,0.05,0.1,0.15,0.2],\n","     'n_estimators':[1,3,5,10,50,100,250,500,1000],\n","     'max_depth':[3,5,10,15,20,50],\n","    'colsample_bytree':[0.1,0.3,0.5,1],\n","    'subsample':[0.1,0.3,0.5,1]\n","}\n","random_cfl1=RandomizedSearchCV(x_cfl,param_distributions=prams,verbose=10,n_jobs=-1,random_state=25,scoring='f1',\n","                               return_train_score=True)\n","random_cfl1.fit(X_train_merge,y_train)\n","\n","print('mean test scores',random_cfl1.cv_results_['mean_test_score'])\n","print('mean train scores',random_cfl1.cv_results_['mean_train_score'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaiAFWTL7W4p"},"outputs":[],"source":["# printing best parameters and score\n","print(\"Best Parameters: \",random_cfl1.best_params_)\n","print(\"Best Score: \",random_cfl1.best_score_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jp3Dzemz7W4p"},"outputs":[],"source":["# Fitting the model on best parameters\n","xgb = XGBClassifier(n_estimators=50, max_depth=15,subsample=0.5,learning_rate=0.2,colsample_bytree=0.3,random_state=25,\n","                      n_jobs=-1)\n","xgb.fit(X_train_merge,y_train)\n","pickle.dump(xgb,open('models/xgb.pkl','wb'))\n","\n","y_train_pred = xgb.predict(X_train_merge)\n","y_test_pred = xgb.predict(X_test_merge)\n","\n","# printing train and test scores\n","print('Train f1 score',f1_score(y_train,y_train_pred))\n","print('Test f1 score',f1_score(y_test,y_test_pred))"]},{"cell_type":"code","source":["y_test_proba = sgd.predict_proba(X_test_merge)[:, 1]\n","print_evaluation_scores(y_test, y_test_pred, y_test_proba, \"Test\")"],"metadata":{"id":"6luHIbDtpaim"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L87wuuw47W4p"},"outputs":[],"source":["confusion_matrices_plot(y_train,y_train_pred,y_test,y_test_pred)"]},{"cell_type":"markdown","metadata":{"id":"bcDI6VV87W4q"},"source":["### AdaBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fL1ivO5S7W4q"},"outputs":[],"source":["# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n","# Prameter tuning of the LGBM parameters using RandonSearch CV\n","x_cfl=AdaBoostClassifier(random_state=25)\n","\n","prams={\n","    'learning_rate':[0.001,0.01,0.03,0.05,0.1,0.15,0.2],\n","     'n_estimators':[1,3,5,10,50,100,250,500,1000]\n","}\n","random_cfl1=RandomizedSearchCV(x_cfl,param_distributions=prams,verbose=10,n_jobs=-1,random_state=25,scoring='f1',\n","                               return_train_score=True)\n","random_cfl1.fit(X_train_merge,y_train)\n","\n","print('mean test scores',random_cfl1.cv_results_['mean_test_score'])\n","print('mean train scores',random_cfl1.cv_results_['mean_train_score'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hg1ijVrg7W4q"},"outputs":[],"source":["# printing best parameters and score\n","print(\"Best Parameters: \",random_cfl1.best_params_)\n","print(\"Best Score: \",random_cfl1.best_score_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3hDruZQG7W4r"},"outputs":[],"source":["# Fitting the model on best parameters\n","ada = AdaBoostClassifier(n_estimators=500, learning_rate=0.05, random_state=25)\n","ada.fit(X_train_merge,y_train)\n","pickle.dump(ada,open('models/ada_boost.pkl','wb'))\n","\n","y_train_pred = ada.predict(X_train_merge)\n","y_test_pred = ada.predict(X_test_merge)\n","\n","# printing train and test scores\n","print('Train f1 score',f1_score(y_train,y_train_pred))\n","print('Test f1 score',f1_score(y_test,y_test_pred))"]},{"cell_type":"code","source":["y_test_proba = sgd.predict_proba(X_test_merge)[:, 1]\n","print_evaluation_scores(y_test, y_test_pred, y_test_proba, \"Test\")"],"metadata":{"id":"6h2dt0oepbr-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"miiPKfRl7W4r"},"outputs":[],"source":["confusion_matrices_plot(y_train,y_train_pred,y_test,y_test_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foMEr9Qa7W49"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}